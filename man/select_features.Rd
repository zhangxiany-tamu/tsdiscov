% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/feature_selection.R
\name{select_features}
\alias{select_features}
\title{Feature Selection Using Hypothesis Testing}
\usage{
select_features(
  X,
  y,
  fdr_level = 0.05,
  hypotheses_independent = FALSE,
  correction_method = "fdr",
  ml_task = "auto",
  multiclass = FALSE,
  n_significant = 1,
  test_binary_real = "mann"
)
}
\arguments{
\item{X}{Feature matrix (data.frame or matrix with named columns)}

\item{y}{Target vector (numeric, factor, or logical)}

\item{fdr_level}{False discovery rate threshold (default 0.05). Also used as
FWER threshold when correction_method is "bonferroni", "holm", etc.}

\item{hypotheses_independent}{Assume independent features? If FALSE (default),
uses more conservative Benjamini-Yekutieli correction (only for FDR methods)}

\item{correction_method}{Multiple testing correction method. Options:
\itemize{
  \item "fdr": False Discovery Rate using BH or BY (default)
  \item "bonferroni": FWER control, very conservative, robust to any dependence
  \item "holm": FWER control, less conservative than Bonferroni
  \item "hochberg": FWER control, assumes non-negative dependence
  \item "hommel": FWER control, more powerful than Hochberg
}}

\item{ml_task}{"auto" (default), "classification", or "regression"}

\item{multiclass}{Is this a multiclass classification problem? (default FALSE)}

\item{n_significant}{Minimum number of classes a feature must be significant
for in multiclass problems (default 1)}

\item{test_binary_real}{Test for binary target + real feature: "mann" (default)
for Mann-Whitney U or "ks" for Kolmogorov-Smirnov}
}
\value{
A list of class 'tsdiscov_feature_selection' with components:
\itemize{
  \item X_selected: Reduced feature matrix with only relevant features
  \item relevance_table: data.frame with detailed results for each feature
  \item n_features_original: Original number of features
  \item n_features_selected: Number of selected features
  \item fdr_level: FDR threshold used
  \item ml_task: Machine learning task (classification or regression)
}
}
\description{
Implements the FRESH (FeatuRE Selection with Hypothesis testing) algorithm
to automatically select relevant features based on statistical significance
tests with False Discovery Rate (FDR) correction.
}
\details{
The FRESH algorithm tests each feature independently for association with
the target variable using appropriate statistical tests:

\itemize{
  \item Binary target + Binary feature: Fisher's exact test
  \item Binary target + Real feature: Mann-Whitney U test (default) or
    Kolmogorov-Smirnov test
  \item Real target + Binary feature: Kolmogorov-Smirnov test
  \item Real target + Real feature: Kendall's tau correlation test
}

Multiple testing correction is applied using the Benjamini-Hochberg (BH)
procedure if hypotheses_independent=TRUE, or the more conservative
Benjamini-Yekutieli (BY) procedure if hypotheses_independent=FALSE (recommended
for correlated features like time series features).
}
\examples{
\dontrun{
# Extract features from time series
library(tsdiscov)
set.seed(123)

# Generate time series with different characteristics
ts_normal <- replicate(50, rnorm(100), simplify = FALSE)
ts_ar <- replicate(50, arima.sim(list(ar = 0.8), 100), simplify = FALSE)

# Extract all features
features_normal <- do.call(rbind, lapply(ts_normal, ts_features))
features_ar <- do.call(rbind, lapply(ts_ar, ts_features))

X <- rbind(features_normal, features_ar)
y <- c(rep(0, 50), rep(1, 50))

# Select relevant features
result <- select_features(X, y, fdr_level = 0.05)

# Examine results
print(result)
head(result$relevance_table)

# Use selected features for modeling
X_selected <- result$X_selected
}
}
\references{
Christ, M., Kempa-Liehr, A.W. and Feindt, M. (2016).
Distributed and parallel time series feature extraction for industrial big data applications.
ArXiv e-prints: 1610.07717

Benjamini, Y. and Hochberg, Y. (1995).
Controlling the false discovery rate: a practical and powerful approach to multiple testing.
Journal of the Royal Statistical Society. Series B, 57(1), 289-300.
}
